{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG from Scratch: Query Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGCHAIN_API_KEY=os.getenv('LANGCHAIN_API_KEY')\n",
    "LANGCHAIN_TRACING_V2=os.getenv('LANGCHAIN_TRACING_V2')\n",
    "LANGCHAIN_ENDPOINT=os.getenv('LANGCHAIN_ENDPOINT')\n",
    "GROQ_API_KEY=os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "import numpy as np\n",
    "import bs4\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.load import dumps, loads\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c494c809ac4e20a2a8b2ad944d9217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['fcdd3e0d-c502-4f3e-bbdd-8a70d8a59216',\n",
       " 'e0c1c079-5f05-4425-a40f-45f9bcbda6e1',\n",
       " '4fab0542-2526-4818-85bf-b3d838aa7c7c',\n",
       " '09b71a75-c616-42c6-8c99-79d34bd4b74b',\n",
       " 'e6eb3726-88a5-4ca9-838f-e443583bde8c',\n",
       " '114e8045-7c8f-42b4-812e-117001506f47',\n",
       " '967176ff-63b6-4777-bd5f-c029596fd866',\n",
       " 'c61b1b04-dbdf-45ad-93c0-e25380eabaf8',\n",
       " '7469b911-903e-47e0-9c66-5a1d06a6d23c',\n",
       " '1d0fefa6-7d91-4322-b1d7-8d79a449475f',\n",
       " 'fa242fa7-f2ec-4f73-93ad-df01f138af9d',\n",
       " '9dfc5098-1c56-4f7f-84e3-e9f48d4989dd',\n",
       " '2dc60516-795c-4a3b-9879-be6aef285ec1',\n",
       " 'c42164dd-abe4-4537-8e6a-21972844d5e5',\n",
       " '032428a7-87ac-4361-8f71-6e70eac81a25',\n",
       " 'fdeff041-b395-40ca-8f60-485bb6a5c59b',\n",
       " 'fb2d3da2-82e5-45fb-b482-01f26833612d',\n",
       " '8abd8319-2754-4821-b157-413e4f9091df',\n",
       " '54797c9a-6848-4a94-ab1c-60dd8468cdca',\n",
       " '00ac902c-05e0-411e-b2ce-548b7ebbdd5b',\n",
       " 'ff6d8324-24a4-4af1-84b6-71d12b0ad9d2',\n",
       " '75933296-5412-4ba5-a4ee-f015595e7a39',\n",
       " 'f9c93d9d-50ba-4034-9084-75cc498c2fa7',\n",
       " '8b7dffde-15bb-4ff9-9718-7fcee43b0bd6',\n",
       " 'c7623664-3a1b-4911-a349-d354d7cbbe1e',\n",
       " '7b034b32-7256-4cd0-9c58-5ec48ad65bd0',\n",
       " '74f16ac5-2b2d-4ece-9f0a-a155e9d0b033',\n",
       " 'f489520b-206c-4407-898c-e04a1dec9632',\n",
       " 'd581c9b2-f726-4d14-a511-63d800cbfdc0',\n",
       " 'ac89635a-0a7a-4e39-9102-2e3af1960446',\n",
       " 'cc9fb06a-7a5a-4fe8-9a61-8441f1e2c39e',\n",
       " 'f32f820c-74cd-4f1d-9756-46143f8a73c6',\n",
       " 'd7c85d11-d767-4248-a1f9-bdab06544a59',\n",
       " 'e84d8993-498f-4e55-bc2c-23ffa20627ff',\n",
       " '97c6e453-f604-4471-85fe-cd5e09d276fe',\n",
       " 'ca91c4e6-b09e-454e-aa41-bc2677d20543',\n",
       " '09908024-acb7-4b53-8d9d-8bb86c46c54d',\n",
       " '478d81e1-bb87-4eb2-b061-17a8c32e8c4b',\n",
       " '4b48f53c-462b-4bff-ac46-59cd68bde487',\n",
       " 'bbabf1ef-73d8-4f83-aa1e-e02f5261b331',\n",
       " '5cf59b47-79ef-45fd-9541-796132fb5360',\n",
       " '90f66f60-d362-47fd-bc9b-4f1381e4f34c',\n",
       " '38f56329-13f9-46be-955a-f088fd7a17a0',\n",
       " '25e5ee56-97fe-44ff-a6a9-3efba6d07c10',\n",
       " 'd8db771c-84ce-491c-8000-281906b404b9',\n",
       " 'c2cb8a0d-11dd-4634-b4fc-f1daf35718ae',\n",
       " '95d3c63c-bbcb-4c22-8850-c48414a30b3f',\n",
       " '4a23f615-c6a0-4e0f-8c12-158c0c46517b',\n",
       " 'd394afe4-07e6-4f08-b4c2-8c1abac9668b',\n",
       " '70d3f411-8bfa-4216-b3b7-134d132212a2',\n",
       " 'd9a3d4e0-f3dc-494b-b15b-dd7911648b51',\n",
       " 'f1bd2cba-8c5e-4427-87dd-d0597a752a87',\n",
       " '6ec9743a-4c9e-4bf6-896b-fc5740d351a3',\n",
       " '0d2cd8bf-2ccb-4d8b-9b89-8678be718ab3',\n",
       " '895140d0-bee7-49ae-bba8-5f4d1adcea6a',\n",
       " 'd59615ec-69e9-4c7f-88b9-df92065b1e99',\n",
       " 'd292d3a1-2e97-4a55-964c-27c7418c5543',\n",
       " '1ed92dfb-171e-4e1f-920f-e9c1bafc548a',\n",
       " '3a936605-e679-4be0-8f5b-52e2b824c9a9',\n",
       " 'ec0f7929-dbaf-4daf-b38d-bb8e04c3611c',\n",
       " '2da5034c-5b99-423f-a3f7-e66cdce0a667',\n",
       " '6b518a13-2445-43b3-9083-6759e5d3358d',\n",
       " '9c8fd894-58da-4275-ad48-b2e41b58f791',\n",
       " '95409c2c-c503-4cd2-a015-b9f8939bbf31',\n",
       " '72751319-013b-4e9c-8fdf-dfb6c6301264',\n",
       " '83e9f74c-b8e3-4b9d-b6cc-e73115c843fd',\n",
       " 'b270fb8d-d191-449e-8ba3-31320ba79e9e',\n",
       " 'f79f6e71-1994-4e1f-b1d0-2ecbcf4a4b16',\n",
       " '9bb47354-32aa-4744-96e1-8bf0fe4ef915',\n",
       " 'b34a2509-b126-4514-a00b-72af7b4fa704',\n",
       " 'ba01b555-52a2-47a6-a339-44e260a8aadf',\n",
       " '529599ad-05e8-49f7-9f14-2d4dcecb144c',\n",
       " '996968d7-1498-4cd7-98b7-60ec0511cbf4',\n",
       " '08134c27-92b6-475f-a48d-900313e13729',\n",
       " '8c35e45f-3b01-47ad-bc65-d505e96a0dec',\n",
       " '32a513ba-b893-4d7b-be8c-f63443e30a20',\n",
       " 'df436e56-2599-4ada-827f-f2a5a8c5ef91',\n",
       " 'a5acd946-793f-4b8d-8441-d8d3be38e4c9',\n",
       " '56580cc0-831a-44ac-9960-629be7d77fd9',\n",
       " '184b862b-ede5-4e40-acec-acea79993937',\n",
       " '36a14592-5781-45bc-a41a-407a399e9d34',\n",
       " '0e638890-b58e-48a6-b04c-aeb69f64d9da',\n",
       " 'd4a6e2d6-794e-43dd-8956-e6f4d90c105f',\n",
       " '1f6b8ff5-525f-4473-a771-30d6f5c0773c',\n",
       " '4d3c4908-42a2-41b2-b5aa-d6e0496119af',\n",
       " '1974614a-06bc-4938-b67b-098aff834e46',\n",
       " '7d0f49f0-6496-40f0-b156-942a6cfa6b80',\n",
       " '6a8310bb-4624-448e-9abe-4886344cda95',\n",
       " '2ffcc4a7-c9bc-45c7-be3e-d671c08a4fdb',\n",
       " '28dded8f-2129-4e90-9983-1c556a03cbce',\n",
       " 'e8270863-a417-4ee0-b297-8691f8ad9946',\n",
       " '5260d961-2612-4a77-a85e-124c46868138',\n",
       " 'c3aef0b3-068f-4dd6-87b7-3e6b731ed270',\n",
       " 'ffc2c6ba-9a81-4c74-a2ad-aef3d8bb7f9f',\n",
       " '1d960002-afb9-4c6b-b5d0-0f7e7df6b405',\n",
       " 'b82d4c0a-6fe8-4d12-ae5b-dee2bc4e7cc4',\n",
       " '5d5a20b2-96aa-4ed0-9ee0-124e1724830a',\n",
       " 'ab83dd7a-af05-4826-85ae-8efa1ebc8557',\n",
       " '49038a50-5010-4e8a-9bbc-840a5d6c2e49',\n",
       " '4b497116-b203-4ccb-9fdb-e2de3e8e5349',\n",
       " 'de9aa24d-6fe2-4309-b9fa-898bb87fe533',\n",
       " '3ca881b9-8344-4bc2-9e9f-9804fbccc33a',\n",
       " '9deddf5e-22ea-445c-8181-7b6aacf267eb',\n",
       " 'd665c07f-38fa-4c84-a532-ab9f3f39dcc6',\n",
       " '84be339c-f7b0-450a-bf24-9ea72a1710b5',\n",
       " '790102aa-7974-4bf1-929b-00d6f1b72639',\n",
       " 'baa8b13d-768d-422e-8d2a-e043c0b26c1f',\n",
       " '9f00875f-8822-461d-9396-80ccc3ab5c74',\n",
       " 'ed6eed39-3548-4da4-afde-862ad7635e1b',\n",
       " 'b7ae723a-fee6-454d-9dab-21260cbfedbc',\n",
       " 'da265a42-72f3-48c0-aac6-cd5da51d31bb',\n",
       " '1464759d-ea7a-4d30-bba4-cca4b5581f11',\n",
       " '5e40445c-d6a6-4f4c-bc3e-d07053d8732b',\n",
       " 'c15a4b07-55db-4f59-b382-d2e8ef9c7c3d',\n",
       " '957997fb-b797-4a94-8a3c-db05734048f7',\n",
       " 'b3a6cb30-14a1-4b3c-882c-4c799827f1b3',\n",
       " '1582ade3-7ff4-4c75-8d1c-021b58ea4b5b',\n",
       " 'e8e1b559-5cbb-4fe3-9e8b-2de9b218937d',\n",
       " 'c1eca6c0-d109-4f99-ab6a-ccc03515571e',\n",
       " '1849fb5b-c2f8-417f-8bed-6bf7a99a5703',\n",
       " '3be2ec7f-ecf4-4656-9b39-67e0ef67a164',\n",
       " '5facf3b6-f6ee-4d4f-851e-5a1e7147ff2a',\n",
       " '34284f86-7d17-401b-b3cc-761e6741243f',\n",
       " 'f0debb2e-6a22-4480-8632-f8591cd0b2ee',\n",
       " '71202e63-8a77-4625-b4ec-96590931fc4a',\n",
       " '26195d0d-5c12-446a-85db-ac8f162ca615',\n",
       " 'ab329984-e4cc-4cf2-aacd-97d52cec0908',\n",
       " '0ecd4702-2fd7-482a-bb6c-f66f45da6b2e',\n",
       " 'e532f0b7-d7e7-473d-b6b1-37805838e677',\n",
       " 'b77ceb46-7a24-417e-aa65-6fb3b66aa768',\n",
       " '4e6dd678-61ed-427f-a36a-9c082fb46038',\n",
       " 'd0ffe9f2-a5b3-49b1-a18d-1d57f161ccdd',\n",
       " '6505828b-4ac7-4a40-8e03-f1c46a8ad4ed',\n",
       " '9e5b471c-cddc-4d12-b7a2-7b1b84932fec',\n",
       " '9e388bbb-f5be-4acb-9c05-e974eee4a097',\n",
       " '54a53be6-0175-4511-acdc-66851da81da2',\n",
       " '87e7afbb-47ef-4ac4-8064-9227d78f9af4',\n",
       " '793c143a-2238-474b-a46f-f2ec33a42153',\n",
       " '8f36a357-522c-4a83-b548-5c504fa50dbb',\n",
       " '5fdb2c85-3e50-4754-bf4e-6638880f94d8',\n",
       " '7522b66b-322b-4940-8458-0bb86b78a631',\n",
       " '3d1d7318-abde-463a-952e-dadc94964914',\n",
       " '5630acb1-f265-4e73-ab5f-08ef0fb019cf',\n",
       " '524148a7-bf86-4329-a832-9af4e4ce9692',\n",
       " '9abf365a-ff02-4f45-8434-090b2ea68e61',\n",
       " '437f2316-b7fc-4740-bbd4-86957836c90f',\n",
       " '0fa55188-d138-4947-aa73-fa569bd88b74',\n",
       " 'a16bb8e0-bb91-45b2-a05c-94c4ff226270',\n",
       " '7f8d2ef7-d503-48e0-8a25-c0d0c2f061ef',\n",
       " 'd81193dc-a7ba-4030-9067-a3e3d1cc1dfd',\n",
       " '0626c8b7-b1c2-499c-9af4-7cf60c8d79c6',\n",
       " '8ed4a6ac-a2dc-4a82-8091-d8dcdff74a1b',\n",
       " '5258a895-a1ba-4d82-906e-c862aa51fdb1',\n",
       " '32558906-bfb5-4c71-947e-f0fe87d91f9a',\n",
       " '626421be-b9b6-40b6-b18b-15d8d6aa0d06',\n",
       " '367b1873-7b41-48f6-8dab-5b76465eece6',\n",
       " '00d9c8ca-11f5-4506-bb5d-3ce36891c16c',\n",
       " 'a7e2d143-9bc3-4267-a335-eb0da46d98b6',\n",
       " '58cc0e01-7022-4212-9e5a-b55595abdf35',\n",
       " '2350e1cd-bdbb-4a00-85a6-277371d4b9b3',\n",
       " '396ebb4d-5d0d-45f9-90a9-fda36937368e',\n",
       " '0be83b37-9e94-40bc-a29b-b5feb035461c',\n",
       " '8b9e5978-649a-4931-ab78-0927ab807fd5',\n",
       " 'c7764cd7-c0b8-4977-8ca6-c5fbff4d02f9',\n",
       " '65f1cfdc-e47c-466a-a4be-4f19937f167f',\n",
       " 'c5bb154d-2e41-462f-bd9f-e795c332b62e',\n",
       " '1ce9b9cc-e513-4f6b-b184-53f34a7ee6a7',\n",
       " 'f0cba252-5aaf-4d44-a272-769e19391bb0',\n",
       " '6c3f1ebc-24c9-4b5d-b7b6-0ac3a856b73b',\n",
       " 'fc87d8fd-cb56-449f-b634-596b46fa1da7',\n",
       " 'aa85684a-6ed0-49e9-8eab-f399007231d6',\n",
       " 'e1a78cc4-70ee-47f9-b5fc-496cc5a4f4a7',\n",
       " '0ed29243-5559-49f1-aefd-93f4b40da5cf',\n",
       " '08efa4c2-2555-4af9-b99f-f4bd913db1f9',\n",
       " '53046a9e-4c85-4180-a518-bf6b6e9043a2',\n",
       " '703d6442-2139-434e-87f8-887dec89df11',\n",
       " 'e852fb44-0c37-4261-8e71-b00e1636c0ff',\n",
       " '79c11106-80a3-47b5-8918-fbb36867eb48',\n",
       " '11d547d1-41f4-4276-9887-91859f256e49',\n",
       " '4219fd36-8d2f-422b-8f57-2fe320091a6f',\n",
       " 'c8ad74cc-d2c4-46d7-814f-5636e8696b74',\n",
       " 'adb842c9-110e-4ec5-b4a0-c9636725e806',\n",
       " '71f2d4e2-def1-418d-8d53-8e358f56d0b7',\n",
       " '7d78966c-4b73-4e5a-a2a2-c7caa4147900',\n",
       " '6f8790b6-5909-44ae-aebb-0d7f1a88a0bb',\n",
       " '73541376-2a8c-42d7-9ad6-6f5b13d76490',\n",
       " '37185ee8-a62e-458f-8dbd-13b083fe5516',\n",
       " '99dc87c0-dc25-48f1-9ec9-82e62e174c00',\n",
       " 'a9bf18fa-08ca-4315-b32e-e84074e18e3c',\n",
       " '6321b559-dc2e-41a3-931b-c3eac3eaaa6a',\n",
       " 'c4ea711a-1a49-44f0-9817-49ca5ea0e638',\n",
       " '788cf96a-b08f-4df1-8980-dbcf2e87fd08',\n",
       " '14e9a6f6-7458-4123-a797-dee4690b154c',\n",
       " '965f65d4-1b60-4291-9981-ef47fcb10075',\n",
       " 'e9c10819-573a-45ec-bfaa-80e7939fa5e4',\n",
       " '4b8f4c3d-d2f2-4ef2-8bf2-cb3635536c4c',\n",
       " '5a95b469-7804-43be-a7c4-77df7ced72c2',\n",
       " '3ab6bf36-5df0-434f-b0bc-c73c3ebc1d71',\n",
       " 'd7f43bc9-6a75-4b6a-86a3-512ec30aa09b',\n",
       " '51b1bd1d-0bb9-4f1e-9d90-6bfa68b9b115',\n",
       " 'c738276d-97f7-451e-a111-26d1f2882479',\n",
       " '637ca904-7155-428d-a06d-27c963bf426e',\n",
       " '26bab274-9168-41ed-a840-185b009d6154',\n",
       " '8449a02f-4b4e-45d8-a295-ba8f1d46fc23',\n",
       " 'db361d10-5391-4c5b-9ce4-23816140522a',\n",
       " '4a3d82e7-e28a-44d6-a3e2-52c78e69f79b',\n",
       " 'ca7df72a-9272-42fc-b28d-01574faea89b',\n",
       " '093e0035-d886-48d1-a597-9a0d68e511ff',\n",
       " 'fee908a3-4b77-43ec-9927-494569f4c682',\n",
       " '8f5c4ce1-319d-4d1d-8f70-2ac539d72646',\n",
       " 'db2ba64d-8096-4fab-98b4-81b07269b6ac',\n",
       " 'b47ca970-c910-4719-b35d-eb576e57afda']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()\n",
    "\n",
    "# splitting the document using text splitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "splits = splitter.split_documents(blog_docs)\n",
    "\n",
    "# initializing the vector store\n",
    "\n",
    "vectordb = Chroma(embedding_function=FastEmbedEmbeddings())\n",
    "retriever = vectordb.as_retriever(kwargs={\"k\":1})\n",
    "retriever.add_documents(splits) # indexing documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Rewriting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main motivation for Query Translation is that user queries can be ambiguous, which can negatively impact the retrieval process, as the relevant document will, possibly, not be able to be retrieved. One way to solve is Query Rewriting. In this type of query translation, we will use an LLM to generate multiple forms of rewritten queries from a given user query, and we will use those queries to retrieve relevant documents. There are two types of Query Rewriting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Query Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the generating multiple queries and performing retrieval, we will just compile the documents, and then give them to the LLM as context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the prompt template\n",
    "\n",
    "prompt = \"\"\"You are an AI language model assistant. Your task is to generate five different versions of the given user question to retrieve relevant documents from a vector database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search. Only provide the alternative questions separated by newlines and no spaces should be between the questions. Original question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the chain for Multiquery Retrieval\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives\n",
    "    | ChatGroq(model=\"llama-3.1-70b-versatile\", temperature=0.0)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: [y for y in x.split('\\n') if y.strip()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to return a unique set of documents from the given retrieved documents\n",
    "\n",
    "def unique_docs(documents: list[list]):\n",
    "    # flatten list of lists and convert each Document into a string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    unique = list(set(flattened_docs))\n",
    "\n",
    "    return [loads(doc) for doc in unique]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrating the MultiQuery chain\n",
    "\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "retrieval_chain = (\n",
    "    generate_queries\n",
    "    | retriever.map()\n",
    "    | unique_docs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shahe\\AppData\\Local\\Temp\\ipykernel_17400\\3134883290.py:8: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  return [loads(doc) for doc in unique]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# invoke the chain to retrieve the documents\n",
    "\n",
    "docs = retrieval_chain.invoke({\"question\": question})\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final RAG chain\n",
    "\n",
    "template = \"\"\"Answer the question. This context is provided to help you answer the question:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "llm = ChatGroq(model=\"llama-3.1-70b-versatile\", temperature=0.0)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain,\n",
    "     \"question\": itemgetter(\"question\")}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM (Large Language Model) agents is the process of breaking down large tasks into smaller, manageable subgoals. This enables the agent to efficiently handle complex tasks. Task decomposition can be done in several ways, including:\\n\\n1. By LLM with simple prompting, such as asking for steps or subgoals for a specific task.\\n2. By using task-specific instructions, such as writing a story outline for a novel.\\n3. With human inputs, where humans provide guidance on how to break down a task into smaller subgoals.\\n\\nTask decomposition is an important component of the planning stage in a LLM-powered autonomous agent system, as it allows the agent to plan ahead and execute complex tasks in a more efficient and effective manner.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_rag_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAG Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to Multiquery retrieval, but we will be performing an additional processing step of using Reciprocal Reranking Function (RRF) to rerank the retrieved documents into a single unified ranking, which the LLM will use to generate the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are an AI language model assistant. Your task is to generate four different versions of the given user question to retrieve relevant documents from a vector database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search. Only provide the alternative questions separated by newlines and no spaces should be between them. Original question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_queries_rag_fusion = (\n",
    "    prompt_rag_fusion\n",
    "    | ChatGroq(model=\"llama-3.1-8b-instant\")\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: [y for y in x.split('\\n') if y.strip()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the function for RRF\n",
    "\n",
    "def reciprocal_ranking_function(documents: list[list], k=60):\n",
    "    fused_scores = {} # fused scores, on which the documents will be reranked\n",
    "\n",
    "    for docs in documents:\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc) # converting docs to a string\n",
    "\n",
    "            if doc_str not in fused_scores: # if the doc is not already in the fused_scores dictionary\n",
    "                fused_scores[doc_str] = 0 # assign the minimum score for now\n",
    "\n",
    "            previous_score = fused_scores[doc_str] # extract the previous score\n",
    "            fused_scores[doc_str] += 1 / (k + rank) # add the calculated score\n",
    "\n",
    "    ranked_results = [\n",
    "        (loads(doc), score) for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True) # rerank the documents\n",
    "    ]\n",
    "\n",
    "    return ranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrating the RAG fusion chain\n",
    "\n",
    "retrieval_chain_rag_fusion = (\n",
    "    generate_queries_rag_fusion\n",
    "    | retriever.map()\n",
    "    | reciprocal_ranking_function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieving the documents\n",
    "\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final RAG fusion chain\n",
    "\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.0)\n",
    "\n",
    "final_rag_fusion_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion,\n",
    "     \"question\": itemgetter(\"question\")}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM (Large Language Model) agents involves breaking down large tasks into smaller, manageable subgoals. This enables efficient handling of complex tasks. According to the provided context, task decomposition can be done in three ways:\\n\\n1. By LLM with simple prompting, such as \"Steps for XYZ.\\\\\\\\n1.\" or \"What are the subgoals for achieving XYZ?\"\\n2. By using task-specific instructions, for example, \"Write a story outline\" for writing a novel.\\n3. With human inputs.\\n\\nThis process is mentioned in the context as a way to facilitate long-term planning and task management for LLM agents.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_rag_fusion_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Decomposition"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_from_scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
